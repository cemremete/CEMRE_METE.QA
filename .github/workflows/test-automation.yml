name: Insider Test Automation Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      browser:
        description: 'Browser to test'
        required: true
        default: 'chrome'
        type: choice
        options:
        - chrome
        - firefox
        - safari
        - all
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'critical'
        type: choice
        options:
        - critical
        - smoke
        - regression
        - performance
        - cross_browser
        - all

jobs:
  test-matrix:
    runs-on: ubuntu-latest
    env:
      DISPLAY: :99
      PYTHONPATH: ${{ github.workspace }}
      ACTIONS_STEP_DEBUG: true
      ACTIONS_RUNNER_DEBUG: true
    strategy:
      fail-fast: false
      matrix:
        browser: [chrome, firefox]
        test-type: [critical, performance]
        include:
          - browser: chrome
            test-type: cross_browser
          - browser: firefox
            test-type: cross_browser

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Fix package compatibility issues
      run: |
        # Install modern packages first
        sudo apt-get update
        sudo apt-get install -y libasound2t64 libgtk-3-0 libx11-xcb1 libxcomposite1 libxcursor1 libxdamage1 libxi6 libxtst6 libnss3 libcups2 libxss1 libxrandr2 libpangocairo-1.0-0 libatk1.0-0 libcairo-gobject2 libgdk-pixbuf2.0-0
        
        # Create dummy packages for deprecated dependencies
        sudo apt-get install -y equivs
        
        # Create dummy libasound2 package
        echo "Package: libasound2" > libasound2-dummy
        echo "Version: 1.0" >> libasound2-dummy
        echo "Architecture: all" >> libasound2-dummy
        echo "Maintainer: CI <ci@example.com>" >> libasound2-dummy
        echo "Description: Dummy package to replace deprecated libasound2" >> libasound2-dummy
        echo "Depends: libasound2t64" >> libasound2-dummy
        equivs-build libasound2-dummy
        sudo dpkg -i libasound2_1.0_all.deb || true
        
        # Create dummy libgconf-2-4 package
        echo "Package: libgconf-2-4" > libgconf-dummy
        echo "Version: 1.0" >> libgconf-dummy
        echo "Architecture: all" >> libgconf-dummy
        echo "Maintainer: CI <ci@example.com>" >> libgconf-dummy
        echo "Description: Dummy package to replace deprecated libgconf-2-4" >> libgconf-dummy
        equivs-build libgconf-dummy
        sudo dpkg -i libgconf-2-4_1.0_all.deb || true

    - name: Verify setup
      run: |
        echo "=== System Information ==="
        echo "OS: $(lsb_release -d)"
        echo "Kernel: $(uname -r)"
        echo "Architecture: $(uname -m)"
        echo "Python version: $(python --version)"
        echo "Pip version: $(pip --version)"
        echo "Working directory: $(pwd)"
        echo "=== Environment Variables ==="
        echo "DISPLAY: $DISPLAY"
        echo "PYTHONPATH: $PYTHONPATH"
        echo "=== Directory Contents ==="
        ls -la
        echo "=== Browser Detection ==="
        which google-chrome && google-chrome --version || echo "Chrome not found"
        which firefox && firefox --version || echo "Firefox not found"
        echo "=== Package Status ==="
        dpkg -l | grep -E "(libgtk|libx11|libnss|libgconf)" || echo "GUI packages not found"

    - name: Install Chrome
      if: matrix.browser == 'chrome'
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        google-chrome --version

    - name: Install Firefox
      if: matrix.browser == 'firefox'
      run: |
        sudo apt-get update
        sudo apt-get install -y firefox
        firefox --version

    - name: Install Xvfb for headless support
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb

    - name: Create directories
      run: |
        mkdir -p screenshots/failed screenshots/passed screenshots/evidence
        mkdir -p reports/html reports/json
        chmod -R 755 screenshots/ reports/

    - name: Start Xvfb
      run: |
        echo "=== Starting Xvfb ==="
        sudo Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
        sleep 3
        echo "=== Xvfb Process Check ==="
        ps aux | grep Xvfb || echo "Xvfb not running"
        echo "=== Display Test ==="
        export DISPLAY=:99
        xdpyinfo || echo "Display not accessible"

    - name: Run Critical Tests
      if: matrix.test-type == 'critical'
      run: |
        echo "=== Running Critical Tests on ${{ matrix.browser }} ==="
        echo "Test command: pytest tests/ --browser=${{ matrix.browser }} --headless -m critical"
        echo "=== Pre-test Environment ==="
        ps aux | grep -E "(chrome|firefox|Xvfb)" || echo "No browser processes"
        echo "=== Running Tests ==="
        pytest tests/ \
          --browser=${{ matrix.browser }} \
          --headless \
          -m critical \
          --html=reports/html/critical-${{ matrix.browser }}-report.html \
          --self-contained-html \
          --json-report \
          --json-report-file=reports/json/critical-${{ matrix.browser }}-report.json \
          -v \
          --tb=long \
          --capture=no \
          --log-cli-level=DEBUG
        echo "=== Post-test Process Check ==="
        ps aux | grep -E "(chrome|firefox)" || echo "No browser processes after test"
      continue-on-error: true

    - name: Run Performance Tests
      if: matrix.test-type == 'performance'
      run: |
        echo "=== Running Performance Tests on ${{ matrix.browser }} ==="
        echo "Test command: pytest tests/ --browser=${{ matrix.browser }} --headless -m performance"
        pytest tests/ \
          --browser=${{ matrix.browser }} \
          --headless \
          -m performance \
          --html=reports/html/performance-${{ matrix.browser }}-report.html \
          --self-contained-html \
          --json-report \
          --json-report-file=reports/json/performance-${{ matrix.browser }}-report.json \
          -v \
          --tb=long \
          --capture=no \
          --log-cli-level=DEBUG
      continue-on-error: true

    - name: Run Cross-Browser Tests
      if: matrix.test-type == 'cross_browser'
      run: |
        echo "=== Running Cross-Browser Tests on ${{ matrix.browser }} ==="
        echo "Test command: pytest tests/ --browser=${{ matrix.browser }} --headless -m cross_browser"
        pytest tests/ \
          --browser=${{ matrix.browser }} \
          --headless \
          -m cross_browser \
          --html=reports/html/cross-browser-${{ matrix.browser }}-report.html \
          --self-contained-html \
          --json-report \
          --json-report-file=reports/json/cross-browser-${{ matrix.browser }}-report.json \
          -v \
          --tb=long \
          --capture=no \
          --log-cli-level=DEBUG
      continue-on-error: true

    - name: Upload Test Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-reports-${{ matrix.browser }}-${{ matrix.test-type }}
        path: |
          reports/
          screenshots/
        retention-days: 30

    - name: Upload Failed Screenshots
      uses: actions/upload-artifact@v4
      if: failure()
      with:
        name: failed-screenshots-${{ matrix.browser }}-${{ matrix.test-type }}
        path: screenshots/failed/
        retention-days: 30

    - name: Comment PR with Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'reports/json/critical-${{ matrix.browser }}-report.json';

          if (fs.existsSync(path)) {
            const report = JSON.parse(fs.readFileSync(path, 'utf8'));
            const summary = `
            ## Test Results - ${{ matrix.browser }} (${{ matrix.test-type }})

            - **Total Tests**: ${report.summary.total}
            - **Passed**: ${report.summary.passed} ✅
            - **Failed**: ${report.summary.failed} ❌
            - **Skipped**: ${report.summary.skipped} ⏭️
            - **Success Rate**: ${((report.summary.passed / report.summary.total) * 100).toFixed(1)}%

            ${report.summary.failed > 0 ? '⚠️ **Known Issues**: Based on manual testing, failures are expected due to critical bugs (BUG001-BUG010)' : ''}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          }

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Run Bandit Security Scan
      run: |
        pip install bandit[toml]
        bandit -r . -f json -o security-report.json --exclude tests/ || true

    - name: Upload Security Report
      uses: actions/upload-artifact@v4
      with:
        name: security-report
        path: security-report.json

  code-quality:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    - name: Install quality tools
      run: |
        pip install flake8 black pytest-cov

    - name: Run Black formatter check
      run: black --check --diff .
      continue-on-error: true

    - name: Run Flake8 linter
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
      continue-on-error: true

    - name: Run tests with coverage
      run: |
        pytest --cov=pages --cov=utils --cov-report=html --cov-report=term-missing --tb=short
      continue-on-error: true

    - name: Upload Coverage Report
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: htmlcov/

  generate-summary:
    needs: [test-matrix, security-scan, code-quality]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate Test Summary
      run: |
        echo "# Insider Test Automation Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "## Test Execution Results" >> test-summary.md
        echo "" >> test-summary.md

        # Process test reports
        for report in test-reports-*/reports/json/*.json; do
          if [ -f "$report" ]; then
            echo "Processing $report"
            # Add report summary (simplified)
            echo "- Report: $(basename $report)" >> test-summary.md
          fi
        done

        echo "" >> test-summary.md
        echo "## Known Issues from Manual Testing" >> test-summary.md
        echo "" >> test-summary.md
        echo "- **BUG001**: URL parameter dependency (Critical)" >> test-summary.md
        echo "- **BUG002**: Add to Cart not working (Critical)" >> test-summary.md
        echo "- **BUG004**: Mobile 0% functionality (Critical)" >> test-summary.md
        echo "- **BUG005**: Firefox/Safari complete failure (Critical)" >> test-summary.md
        echo "- **BUG006**: Performance 4.49s delay (High)" >> test-summary.md
        echo "- **BUG007**: 90% incorrect product mapping (Critical)" >> test-summary.md
        echo "" >> test-summary.md
        echo "## Production Readiness: ❌ NOT READY" >> test-summary.md
        echo "" >> test-summary.md
        echo "The system requires critical bug fixes before production deployment." >> test-summary.md

    - name: Upload Summary
      uses: actions/upload-artifact@v4
      with:
        name: test-execution-summary
        path: test-summary.md

  notify-teams:
    needs: [generate-summary]
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/main' || github.event_name == 'schedule')
    steps:
    - name: Notify Teams (Mock)
      run: |
        echo "🔔 Test execution completed"
        echo "📊 Results available in artifacts"
        echo "⚠️ System not ready for production due to critical bugs"
        # In real implementation, this would send notifications to Slack/Teams/Email