[tool:pytest]
# Test discovery configuration
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Marker definitions for test categorization
markers =
    # Priority markers
    critical: Critical priority tests (P1) - Must pass for production release
    high: High priority tests (P2) - Important functionality validation
    medium: Medium priority tests (P3) - Standard feature testing
    low: Low priority tests (P4) - Nice-to-have functionality
    
    # Functional markers
    smoke: Smoke tests for quick validation of core functionality
    regression: Regression tests to ensure existing functionality works
    performance: Performance and load tests for system optimization
    cross_browser: Cross-browser compatibility tests across different browsers
    
    # Feature-specific markers
    popup: Popup functionality tests for modal and dialog interactions
    mobile: Mobile and responsive tests for device compatibility
    unit: Unit tests for individual components and functions
    integration: Integration tests for component interactions and workflows
    responsive: Responsive design tests for various screen sizes
    browser: General browser tests for web application functionality
    
    # Browser-specific markers
    browser_chrome: Chrome-specific tests and compatibility validation
    browser_firefox: Firefox-specific tests and compatibility validation
    browser_safari: Safari-specific tests and compatibility validation
    
    # Device and platform markers
    tablet: Tablet device tests for medium-sized screen compatibility
    desktop: Desktop browser tests for full-screen functionality
    android: Android device specific tests
    ios: iOS device specific tests
    
    # Test type markers
    ui: User interface tests for visual and interaction validation
    functional: Functional tests for business logic validation
    negative: Negative test cases for error handling validation
    edge_case: Edge case scenarios for boundary condition testing
    accessibility: Accessibility tests for WCAG compliance
    security: Security tests for vulnerability assessment
    
    # Performance and reliability markers
    memory: Memory leak tests for resource management validation
    slow: Tests that take longer than 30 seconds to execute
    flaky: Tests that may be unstable and require retry mechanisms
    
    # Environment markers
    local: Tests designed to run in local development environment
    ci: Tests optimized for continuous integration environment
    staging: Tests for staging environment validation
    production: Tests safe to run against production environment

# Default pytest options for consistent execution
addopts = 
    --verbose
    --strict-markers
    --tb=short
    --maxfail=10
    --durations=10
    --capture=no
    --disable-warnings
    --color=yes

# Test execution configuration
minversion = 7.0
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning
    ignore::ResourceWarning
    error::FutureWarning

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Test timeout configuration
timeout = 300
timeout_method = thread

# Coverage configuration (if pytest-cov is installed)
# addopts = --cov=pages --cov=utils --cov-report=html --cov-report=term-missing --cov-report=xml

# Parallel execution configuration (if pytest-xdist is installed)
# addopts = -n auto

# HTML reporting configuration (if pytest-html is installed)
# addopts = --html=reports/test_report.html --self-contained-html

# JSON reporting configuration (if pytest-json-report is installed)
# addopts = --json-report --json-report-file=reports/test_report.json

# Allure reporting configuration (if allure-pytest is installed)
# addopts = --alluredir=reports/allure-results

# Test collection configuration
collect_ignore = [
    "setup.py",
    "conftest.py",
    "venv",
    ".venv",
    "env",
    ".env",
    "node_modules",
    ".git",
    "__pycache__",
    "*.pyc",
    ".pytest_cache",
    ".coverage",
    "htmlcov",
    "reports",
    "screenshots",
    "logs"
]

# Custom test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_*

# Test execution order configuration
# addopts = --order-dependencies --order-group-scope=module

# Benchmark configuration (if pytest-benchmark is installed)
# addopts = --benchmark-only --benchmark-sort=mean

# Memory profiling configuration (if pytest-memray is installed)
# addopts = --memray --memray-bin-path=reports/memray

# Database configuration for test isolation
# addopts = --reuse-db --nomigrations

# Distributed testing configuration
# addopts = --dist=loadscope --tx=popen//python=python3.8

# Test retry configuration (if pytest-rerunfailures is installed)
# addopts = --reruns=2 --reruns-delay=1

# Test selection optimization
# addopts = --lf --ff

# Custom markers for specific test scenarios
# markers =
#     integration_db: Tests that require database connectivity
#     integration_api: Tests that require external API connectivity
#     integration_file: Tests that require file system operations
#     requires_network: Tests that require network connectivity
#     requires_gpu: Tests that require GPU acceleration
#     requires_docker: Tests that require Docker environment