# Insider Test Automation Project - Onsite Experiment Pop-up

## ğŸ¯ Project Overview

This project contains automated test suite for Insider company's "Onsite Experiment" pop-up campaign. The project implements comprehensive test automation using Python, Selenium WebDriver, and Page Object Model (POM) design pattern.

### ğŸ“Š Automated Test Results Summary

- **Test URL:** https://piratesquad.rocks/?campBuilderTest=1&insBuild=MTYwODc=&insVar=YzE3MjU=&routeAlias=custom&queryHash=77e2ecf59905618c6426e9aa8cf7407c47293623d7496e3148944c81aeffce95
- **Total Automated Tests:** 13 scenarios
- **âœ… PASSED:** 3 tests (23.1%)
- **âŒ FAILED (Bug Reproduction):** 6 tests (46.2%)
- **â­ï¸ SKIPPED:** 4 tests (30.8%)
- **ğŸ”„ Test Execution Time:** 5 minutes 40 seconds

### ğŸš¨ Production Readiness: **NO-GO**

**Critical Issues Reproduced:**
- âœ… BUG001: URL parameter dependency (System unusable)
- âœ… BUG002: Add to Cart functionality broken (Revenue impact)
- âœ… BUG004: Mobile/Tablet 0% functionality (60%+ users affected)
- âœ… BUG005: Firefox/Safari complete failure (40%+ browsers affected)
- âš ï¸ Performance issues: Load time exceeds standards
- âš ï¸ Content mapping issues detected

## ğŸ—ï¸ Project Structure

```
insider_test_automation/
â”œâ”€â”€ pages/                          # Page Object Model classes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_page.py                # Base page with common methods
â”‚   â”œâ”€â”€ popup_page.py               # Pop-up specific methods
â”‚   â””â”€â”€ product_page.py             # Product page methods
â”œâ”€â”€ tests/                          # Test files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                 # Pytest configuration
â”‚   â”œâ”€â”€ test_popup_functionality.py # Core functionality tests
â”‚   â”œâ”€â”€ test_popup_performance.py   # Performance tests
â”‚   â””â”€â”€ test_cross_browser.py       # Browser compatibility tests
â”œâ”€â”€ utils/                          # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ driver_manager.py           # WebDriver management
â”‚   â”œâ”€â”€ screenshot_helper.py        # Screenshot utilities
â”‚   â””â”€â”€ test_data.py                # Test data management
â”œâ”€â”€ screenshots/                    # Test screenshots
â”‚   â”œâ”€â”€ failed/                     # Failed test screenshots
â”‚   â”œâ”€â”€ passed/                     # Passed test screenshots
â”‚   â””â”€â”€ evidence/                   # Evidence screenshots
â”œâ”€â”€ reports/                        # Test reports
â”‚   â”œâ”€â”€ html/                       # HTML reports
â”‚   â””â”€â”€ json/                       # JSON reports
â”œâ”€â”€ comprehensive_test_matrix.md    # Complete test case matrix
â”œâ”€â”€ bug_categorization_report.md    # Bug analysis report
â”œâ”€â”€ bug_report_template.md          # Bug reporting template
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ pytest.ini                     # Pytest configuration
â”œâ”€â”€ conftest.py                     # Global test configuration
â”œâ”€â”€ .gitignore                      # Git ignore rules
â””â”€â”€ README.md                       # This file
```

## ğŸ› ï¸ Technology Stack

- **Language:** Python 3.8+
- **Test Framework:** Pytest 7.4+
- **WebDriver:** Selenium 4.15+
- **Design Pattern:** Page Object Model (POM)
- **Reporting:** HTML, JSON, Allure
- **Browser Support:** Chrome, Firefox, Safari
- **CI/CD Ready:** GitHub Actions compatible

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Chrome, Firefox, or Safari browser
- Git

## ğŸš€ Installation & Setup

### 1. Clone the Repository
```bash
git clone <repository-url>
cd insider_test_automation
```

### 2. Create Virtual Environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Verify Installation
```bash
pytest --version
python -c "import selenium; print(f'Selenium version: {selenium.__version__}')"
```

## ğŸ§ª Running Tests

### Basic Test Execution

```bash
# Run all tests
pytest tests/test_popup_functionality.py -v

# Run with verbose output and show all output
pytest tests/test_popup_functionality.py -v -s

# Run specific test file (main test suite)
pytest tests/test_popup_functionality.py

# Run tests with specific marker
pytest tests/test_popup_functionality.py -m critical
```

### Browser-Specific Testing

```bash
# Run tests on Chrome (default)
pytest --browser=chrome

# Run tests on Firefox
pytest --browser=firefox

# Run tests on Safari
pytest --browser=safari
```

### Headless Mode

```bash
# Run tests in headless mode
pytest --headless

# Run with custom window size
pytest --window-size=1366,768
```

### Parallel Execution

```bash
# Run tests in parallel (requires pytest-xdist)
pytest -n auto

# Run with specific number of workers
pytest -n 4
```

### Generate Reports

```bash
# Generate HTML report
pytest tests/test_popup_functionality.py --html=reports/test_report.html --self-contained-html

# Generate JSON report
pytest tests/test_popup_functionality.py --json-report --json-report-file=reports/test_report.json

# Generate both reports
pytest tests/test_popup_functionality.py --html=reports/test_report.html --self-contained-html -v
```

## ğŸ“Š Test Categories & Markers

### Priority Markers
- `@pytest.mark.critical` - P1 Critical tests
- `@pytest.mark.high` - P2 High priority tests
- `@pytest.mark.medium` - P3 Medium priority tests
- `@pytest.mark.low` - P4 Low priority tests

### Functional Markers
- `@pytest.mark.smoke` - Smoke tests
- `@pytest.mark.regression` - Regression tests
- `@pytest.mark.performance` - Performance tests
- `@pytest.mark.cross_browser` - Cross-browser tests

### Browser Markers
- `@pytest.mark.browser_chrome` - Chrome-specific tests
- `@pytest.mark.browser_firefox` - Firefox-specific tests
- `@pytest.mark.browser_safari` - Safari-specific tests

### Example Usage
```bash
# Run only critical tests
pytest -m critical

# Run smoke tests on Chrome
pytest -m "smoke and browser_chrome"

# Run all tests except slow ones
pytest -m "not slow"
```

## ğŸ› Known Issues & Bug Status

### Critical Bugs (P1)
| Bug ID | Status | Description | Impact |
|--------|--------|-------------|---------|
| BUG001 | Open | URL parameter dependency | System unusable |
| BUG002 | Open | Add to Cart not working | Revenue impact |
| BUG003 | Open | Price display errors | Revenue/legal risk |
| BUG004 | Open | Mobile 0% functionality | 60%+ users affected |
| BUG005 | Open | Firefox/Safari failure | 40%+ browsers affected |
| BUG007 | Open | 90% incorrect product mapping | Customer confusion |

### High Priority Bugs (P2)
| Bug ID | Status | Description | Impact |
|--------|--------|-------------|---------|
| BUG006 | Open | 4.49s performance delay | UX degradation |
| BUG008 | Open | Close buttons not working | User frustration |
| BUG009 | Open | Memory leak detected | System stability |
| BUG010 | Open | Missing background overlay | UI inconsistency |

## ğŸ“ˆ Test Results Interpretation

### Success Criteria
- **Functional Success Rate:** >90% (Currently: 23.1%)
- **Bug Reproduction Rate:** 100% (Currently: 100% - All critical bugs reproduced)
- **Performance:** <2s load time (Currently: Working within limits for some tests)
- **Cross-Browser:** Chrome, Firefox, Safari working (Currently: Chrome working, Firefox/Safari failing)
- **Mobile Support:** Responsive design functional (Currently: 0% functionality)
- **Test Automation Coverage:** 13 automated test scenarios implemented

### Current Status: ğŸš¨ **FAILED** - Bug Reproduction Successful âœ…
The system is **not ready for production** due to multiple critical failures affecting core business functionality, cross-browser compatibility, and mobile responsiveness.

**Automation Project Status: âœ… COMPLETED**
- All critical bugs successfully reproduced through automation
- Test framework fully functional with POM implementation
- Screenshot mechanism working for evidence collection
- Comprehensive reporting system implemented

## ğŸ”§ Configuration

### Environment Variables
```bash
# Set test environment
export TEST_ENV=test  # dev, test, staging, prod

# Set browser
export BROWSER=chrome  # chrome, firefox, safari

# Enable headless mode
export HEADLESS=true

# Enable parallel execution
export PARALLEL=true

# Screenshot settings
export SCREENSHOT_ON_FAILURE=true
export SCREENSHOT_ON_SUCCESS=false
```

### Custom Configuration
Edit `utils/test_data.py` to modify:
- Test URLs
- Timeout values
- Performance limits
- Mobile device configurations

## ğŸ“¸ Screenshots & Evidence

Screenshots are automatically captured:
- **Failed tests:** `screenshots/failed/`
- **Passed tests:** `screenshots/passed/`
- **Evidence:** `screenshots/evidence/`

Screenshots include:
- Timestamp
- Test name
- Failure reason (for failed tests)
- Browser information

## ğŸš€ CI/CD Integration

### GitHub Actions Example
```yaml
name: Test Automation

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        browser: [chrome, firefox]

    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.9

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Run tests
      run: |
        pytest --browser=${{ matrix.browser }} --headless -v

    - name: Upload screenshots
      uses: actions/upload-artifact@v2
      if: failure()
      with:
        name: screenshots-${{ matrix.browser }}
        path: screenshots/
```

## ğŸ“ Contributing

### Adding New Tests
1. Create test file in `tests/` directory
2. Follow naming convention: `test_*.py`
3. Use appropriate markers for categorization
4. Include docstrings with test descriptions
5. Add test to comprehensive test matrix

### Adding New Page Objects
1. Create page class in `pages/` directory
2. Inherit from `BasePage`
3. Follow POM principles
4. Include comprehensive docstrings
5. Add element locators as class constants

### Bug Reporting
Use the template in `bug_report_template.md` for consistent bug reporting.

## ğŸ†˜ Troubleshooting

### Common Issues

**WebDriver Issues:**
```bash
# Update WebDriver
pip install --upgrade webdriver-manager

# Clear WebDriver cache
rm -rf ~/.wdm
```

**Permission Issues:**
```bash
# Fix permissions (macOS/Linux)
chmod +x venv/bin/activate

# Run as administrator (Windows)
# Right-click command prompt -> "Run as administrator"
```

**Import Errors:**
```bash
# Ensure virtual environment is activated
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```

### Debug Mode
```bash
# Run with debug output
pytest -v -s --tb=long

# Run single test with debug
pytest tests/test_popup_functionality.py::test_popup_display -v -s
```

## ğŸ“ Support

For issues and questions:
1. Check existing issues in bug report
2. Review troubleshooting section
3. Check test documentation
4. Contact QA team

## ğŸ“„ License

This project is proprietary to Insider company for internal testing purposes.

---

**Last Updated:** September 1, 2025
**Version:** 1.0.0
**Test Execution Date:** September 1, 2025
**Maintainer:** QA Automation Team
**Project Status:** âœ… COMPLETED - All bugs successfully reproduced
